import pandas as pd
import numpy as np
import jieba
import jieba.analyse
from snownlp import SnowNLP
import re
import os
import glob
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class BilibiliUserProfiler:
    def __init__(self):
        self.df = None
        self.user_features = None
        self.tfidf_vectorizer = None
        self.cluster_labels = None
        self.stop_words = self._load_stop_words()

    def _load_stop_words(self):
        """åŠ è½½åœç”¨è¯è¡¨"""
        # å¸¸ç”¨ä¸­æ–‡åœç”¨è¯
        stop_words = [
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'å°±æ˜¯', 'ä½†æ˜¯', 'å¦‚æœ', 'å¯ä»¥', 'å› ä¸º', 'æ‰€ä»¥', 'è™½ç„¶',
            'å“ˆå“ˆ', 'å‘µå‘µ', 'å˜¿å˜¿', 'å•Š', 'å“¦', 'å‘€', 'å§', 'å‘¢', 'å—', 'å—¯', 'å“‡'
        ]
        return set(stop_words)

    def load_data(self, data_dir='./data'):
        """æ‰¹é‡åŠ è½½dataç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        print(f"æ­£åœ¨ä» {data_dir} ç›®å½•åŠ è½½CSVæ–‡ä»¶...")

        # è·å–æ‰€æœ‰CSVæ–‡ä»¶è·¯å¾„
        csv_files = glob.glob(os.path.join(data_dir, '*.csv'))

        if not csv_files:
            raise FileNotFoundError(f"åœ¨ {data_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°CSVæ–‡ä»¶")

        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")
        for file in csv_files:
            print(f"  - {os.path.basename(file)}")

        # æ‰¹é‡è¯»å–å¹¶åˆå¹¶æ‰€æœ‰CSVæ–‡ä»¶
        dataframes = []
        total_rows = 0

        for csv_file in csv_files:
            try:
                df_temp = pd.read_csv(csv_file, encoding='utf-8')
                # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ¹é…é¢„æœŸæ ¼å¼
                expected_columns = ['åºå·', 'ä¸Šçº§è¯„è®ºID', 'è¯„è®ºID', 'ç”¨æˆ·ID', 'ç”¨æˆ·å', 'ç”¨æˆ·ç­‰çº§',
                                    'æ€§åˆ«', 'è¯„è®ºå†…å®¹', 'è¯„è®ºæ—¶é—´', 'å›å¤æ•°', 'ç‚¹èµæ•°', 'ä¸ªæ€§ç­¾å',
                                    'IPå±åœ°', 'æ˜¯å¦æ˜¯å¤§ä¼šå‘˜', 'å¤´åƒ']

                # å¦‚æœåˆ—åä¸å®Œå…¨åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…æˆ–ä½¿ç”¨ç°æœ‰åˆ—
                if not all(col in df_temp.columns for col in expected_columns[:8]):  # è‡³å°‘éœ€è¦å‰8åˆ—
                    print(f"è­¦å‘Š: {os.path.basename(csv_file)} çš„åˆ—åå¯èƒ½ä¸åŒ¹é…ï¼Œå°†å°è¯•å¤„ç†...")
                    print(f"å®é™…åˆ—å: {list(df_temp.columns)}")

                dataframes.append(df_temp)
                rows = len(df_temp)
                total_rows += rows
                print(f"  âœ“ {os.path.basename(csv_file)}: {rows} æ¡è¯„è®º")

            except Exception as e:
                print(f"  âœ— åŠ è½½ {os.path.basename(csv_file)} å¤±è´¥: {str(e)}")
                continue

        if not dataframes:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•CSVæ–‡ä»¶")

        # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
        self.df = pd.concat(dataframes, ignore_index=True)

        # å»é‡å¤„ç†ï¼ˆåŸºäºè¯„è®ºIDï¼‰
        if 'è¯„è®ºID' in self.df.columns:
            original_len = len(self.df)
            self.df = self.df.drop_duplicates(subset=['è¯„è®ºID'], keep='first')
            removed_duplicates = original_len - len(self.df)
            if removed_duplicates > 0:
                print(f"å»é™¤é‡å¤è¯„è®º: {removed_duplicates} æ¡")

        print(f"\næ•°æ®åŠ è½½å®Œæˆ!")
        print(f"æ€»è®¡: {len(self.df)} æ¡è¯„è®ºï¼Œæ¥è‡ª {len(csv_files)} ä¸ªæ–‡ä»¶")
        print(f"æ¶‰åŠç”¨æˆ·æ•°: {self.df['ç”¨æˆ·ID'].nunique() if 'ç”¨æˆ·ID' in self.df.columns else 'æœªçŸ¥'}")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\næ•°æ®æ¦‚è§ˆ:")
        print(self.df.info())

        return self.df

    def load_single_file(self, csv_file):
        """åŠ è½½å•ä¸ªCSVæ–‡ä»¶ï¼ˆä¿ç•™æ­¤æ–¹æ³•ä»¥ä¾¿å‘åå…¼å®¹ï¼‰"""
        self.df = pd.read_csv(csv_file)
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(self.df)}æ¡è¯„è®º")
        return self.df

    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ï¼šä¸­æ–‡åˆ†è¯ã€å»åœç”¨è¯ã€å»å™ªå£°"""
        if pd.isna(text):
            return ""

        # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z]', ' ', str(text))

        # ä¸­æ–‡åˆ†è¯
        words = jieba.cut(text)

        # å»åœç”¨è¯å’Œé•¿åº¦è¿‡çŸ­çš„è¯
        words = [word.strip() for word in words
                 if word.strip() not in self.stop_words and len(word.strip()) > 1]

        return ' '.join(words)

    def extract_sentiment(self, text):
        """æå–æƒ…æ„Ÿææ€§å¾—åˆ†"""
        if pd.isna(text) or text == "":
            return 0.5

        try:
            s = SnowNLP(str(text))
            return s.sentiments
        except:
            return 0.5

    def data_preprocessing(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # å¤„ç†ç¼ºå¤±å€¼
        self.df['è¯„è®ºå†…å®¹'] = self.df['è¯„è®ºå†…å®¹'].fillna('')
        self.df['ä¸ªæ€§ç­¾å'] = self.df['ä¸ªæ€§ç­¾å'].fillna('')
        self.df['ç‚¹èµæ•°'] = pd.to_numeric(self.df['ç‚¹èµæ•°'], errors='coerce').fillna(0)
        self.df['å›å¤æ•°'] = pd.to_numeric(self.df['å›å¤æ•°'], errors='coerce').fillna(0)

        # æ–‡æœ¬é¢„å¤„ç†
        self.df['å¤„ç†åè¯„è®º'] = self.df['è¯„è®ºå†…å®¹'].apply(self.preprocess_text)
        self.df['å¤„ç†åç­¾å'] = self.df['ä¸ªæ€§ç­¾å'].apply(self.preprocess_text)

        # æƒ…æ„Ÿåˆ†æ
        self.df['æƒ…æ„Ÿå¾—åˆ†'] = self.df['è¯„è®ºå†…å®¹'].apply(self.extract_sentiment)

        # å¤„ç†æ—¶é—´
        self.df['è¯„è®ºæ—¶é—´'] = pd.to_datetime(self.df['è¯„è®ºæ—¶é—´'], errors='coerce')

        print("æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return self.df

    def feature_engineering(self):
        """ç‰¹å¾å·¥ç¨‹"""
        print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")

        # ç”¨æˆ·ç»´åº¦èšåˆ
        user_stats = self.df.groupby('ç”¨æˆ·ID').agg({
            'è¯„è®ºå†…å®¹': 'count',  # è¯„è®ºæ•°é‡
            'ç‚¹èµæ•°': 'sum',  # æ€»ç‚¹èµæ•°
            'å›å¤æ•°': 'sum',  # æ€»å›å¤æ•°
            'æƒ…æ„Ÿå¾—åˆ†': 'mean',  # å¹³å‡æƒ…æ„Ÿå¾—åˆ†
            'å¤„ç†åè¯„è®º': lambda x: ' '.join(x),  # æ‰€æœ‰è¯„è®ºåˆå¹¶
            'ç”¨æˆ·å': 'first',
            'ç”¨æˆ·ç­‰çº§': 'first',
            'æ€§åˆ«': 'first',
            'ä¸ªæ€§ç­¾å': 'first',
            'IPå±åœ°': 'first',
            'æ˜¯å¦æ˜¯å¤§ä¼šå‘˜': 'first'
        }).reset_index()

        user_stats.columns = ['ç”¨æˆ·ID', 'è¯„è®ºæ•°é‡', 'æ€»ç‚¹èµæ•°', 'æ€»å›å¤æ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†',
                              'æ‰€æœ‰è¯„è®º', 'ç”¨æˆ·å', 'ç”¨æˆ·ç­‰çº§', 'æ€§åˆ«', 'ä¸ªæ€§ç­¾å', 'IPå±åœ°', 'æ˜¯å¦æ˜¯å¤§ä¼šå‘˜']

        # è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦
        user_stats['æ´»è·ƒåº¦å¾—åˆ†'] = (user_stats['è¯„è®ºæ•°é‡'] * 0.4 +
                                    user_stats['æ€»ç‚¹èµæ•°'] * 0.3 +
                                    user_stats['æ€»å›å¤æ•°'] * 0.3)

        # æƒ…æ„Ÿå€¾å‘åˆ†ç±»
        user_stats['æƒ…æ„Ÿå€¾å‘'] = user_stats['å¹³å‡æƒ…æ„Ÿå¾—åˆ†'].apply(
            lambda x: 'ç§¯æ' if x > 0.6 else ('æ¶ˆæ' if x < 0.4 else 'ä¸­æ€§')
        )

        # æ´»è·ƒç­‰çº§åˆ†ç±»
        activity_quantiles = user_stats['æ´»è·ƒåº¦å¾—åˆ†'].quantile([0.33, 0.67])
        user_stats['æ´»è·ƒç­‰çº§'] = user_stats['æ´»è·ƒåº¦å¾—åˆ†'].apply(
            lambda x: 'é«˜æ´»è·ƒ' if x > activity_quantiles[0.67] else (
                'ä¸­æ´»è·ƒ' if x > activity_quantiles[0.33] else 'ä½æ´»è·ƒ'
            )
        )

        # TF-IDFç‰¹å¾æå–
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            min_df=2
        )

        # è¿‡æ»¤ç©ºè¯„è®º
        valid_comments = user_stats[user_stats['æ‰€æœ‰è¯„è®º'].str.len() > 0]
        if len(valid_comments) > 0:
            tfidf_features = self.tfidf_vectorizer.fit_transform(valid_comments['æ‰€æœ‰è¯„è®º'])
            tfidf_df = pd.DataFrame(
                tfidf_features.toarray(),
                columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])]
            )
            tfidf_df['ç”¨æˆ·ID'] = valid_comments['ç”¨æˆ·ID'].values
            user_stats = user_stats.merge(tfidf_df, on='ç”¨æˆ·ID', how='left')
            user_stats = user_stats.fillna(0)

        self.user_features = user_stats
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…±ç”Ÿæˆ{len(user_stats)}ä¸ªç”¨æˆ·ç”»åƒ")
        return user_stats

    def user_clustering(self, n_clusters=5):
        """ç”¨æˆ·èšç±»åˆ†æ"""
        print("å¼€å§‹ç”¨æˆ·èšç±»...")

        # é€‰æ‹©æ•°å€¼ç‰¹å¾è¿›è¡Œèšç±»
        feature_cols = ['è¯„è®ºæ•°é‡', 'æ€»ç‚¹èµæ•°', 'æ€»å›å¤æ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'æ´»è·ƒåº¦å¾—åˆ†']

        # æ·»åŠ TF-IDFç‰¹å¾
        tfidf_cols = [col for col in self.user_features.columns if col.startswith('tfidf_')]
        if tfidf_cols:
            feature_cols.extend(tfidf_cols[:20])  # å–å‰20ä¸ªTF-IDFç‰¹å¾

        X = self.user_features[feature_cols].fillna(0)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # K-meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        self.cluster_labels = kmeans.fit_predict(X_scaled)

        # è®¡ç®—èšç±»æ•ˆæœ
        silhouette_avg = silhouette_score(X_scaled, self.cluster_labels)
        print(f"K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.3f}")

        # DBSCANèšç±»å¯¹æ¯”
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        dbscan_labels = dbscan.fit_predict(X_scaled)

        n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
        print(f"DBSCANèšç±»å®Œæˆï¼Œå‘ç°{n_clusters_dbscan}ä¸ªç°‡")

        # ä¿å­˜èšç±»ç»“æœ
        self.user_features['èšç±»æ ‡ç­¾'] = self.cluster_labels
        self.user_features['DBSCANæ ‡ç­¾'] = dbscan_labels

        return self.cluster_labels

    def analyze_clusters(self):
        """åˆ†æèšç±»ç»“æœ"""
        print("\nèšç±»ç»“æœåˆ†æ:")

        for cluster_id in range(max(self.cluster_labels) + 1):
            cluster_users = self.user_features[self.user_features['èšç±»æ ‡ç­¾'] == cluster_id]
            print(f"\nç°‡ {cluster_id} (ç”¨æˆ·æ•°: {len(cluster_users)}):")
            print(f"  å¹³å‡è¯„è®ºæ•°: {cluster_users['è¯„è®ºæ•°é‡'].mean():.1f}")
            print(f"  å¹³å‡ç‚¹èµæ•°: {cluster_users['æ€»ç‚¹èµæ•°'].mean():.1f}")
            print(f"  å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {cluster_users['å¹³å‡æƒ…æ„Ÿå¾—åˆ†'].mean():.3f}")
            print(f"  æ´»è·ƒç­‰çº§åˆ†å¸ƒ: {cluster_users['æ´»è·ƒç­‰çº§'].value_counts().to_dict()}")
            print(f"  æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ: {cluster_users['æƒ…æ„Ÿå€¾å‘'].value_counts().to_dict()}")

    def visualize_results_v2(self):
        """ç»“æœå¯è§†åŒ–"""
        print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # 1. ç”¨æˆ·èšç±»åˆ†å¸ƒ
        plt.figure(figsize=(6, 6))
        cluster_counts = pd.Series(self.cluster_labels).value_counts().sort_index()
        plt.pie(cluster_counts.values, labels=[f'ç°‡{i}' for i in cluster_counts.index], autopct='%1.1f%%')
        plt.title('ç”¨æˆ·èšç±»åˆ†å¸ƒ')
        plt.show()

        # 2. æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ
        plt.figure(figsize=(6, 6))
        sentiment_counts = self.user_features['æƒ…æ„Ÿå€¾å‘'].value_counts()
        plt.bar(sentiment_counts.index, sentiment_counts.values)
        plt.title('æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ')
        plt.xlabel('æƒ…æ„Ÿå€¾å‘')
        plt.ylabel('ç”¨æˆ·æ•°é‡')
        plt.show()

        # 3. æ´»è·ƒç­‰çº§åˆ†å¸ƒ
        plt.figure(figsize=(6, 6))
        activity_counts = self.user_features['æ´»è·ƒç­‰çº§'].value_counts()
        plt.bar(activity_counts.index, activity_counts.values, color=['red', 'orange', 'green'])
        plt.title('æ´»è·ƒç­‰çº§åˆ†å¸ƒ')
        plt.xlabel('æ´»è·ƒç­‰çº§')
        plt.ylabel('ç”¨æˆ·æ•°é‡')
        plt.show()

        # 4. èšç±»æ•£ç‚¹å›¾ (ä½¿ç”¨PCAé™ç»´)
        feature_cols = ['è¯„è®ºæ•°é‡', 'æ€»ç‚¹èµæ•°', 'æ€»å›å¤æ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'æ´»è·ƒåº¦å¾—åˆ†']
        X = self.user_features[feature_cols].fillna(0)

        if len(X) > 1:
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(StandardScaler().fit_transform(X))

            plt.figure(figsize=(8, 6))
            scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=self.cluster_labels, cmap='viridis')
            plt.title('ç”¨æˆ·èšç±»å¯è§†åŒ– (PCA)')
            plt.xlabel('ä¸»æˆåˆ†1')
            plt.ylabel('ä¸»æˆåˆ†2')
            plt.colorbar(scatter)
            plt.show()

        # 5. æ´»è·ƒåº¦ vs æƒ…æ„Ÿå¾—åˆ†æ•£ç‚¹å›¾
        plt.figure(figsize=(8, 6))
        scatter2 = plt.scatter(self.user_features['æ´»è·ƒåº¦å¾—åˆ†'], self.user_features['å¹³å‡æƒ…æ„Ÿå¾—åˆ†'],
                               c=self.cluster_labels, cmap='viridis', alpha=0.6)
        plt.title('æ´»è·ƒåº¦ vs æƒ…æ„Ÿå¾—åˆ†')
        plt.xlabel('æ´»è·ƒåº¦å¾—åˆ†')
        plt.ylabel('å¹³å‡æƒ…æ„Ÿå¾—åˆ†')
        plt.show()

        # 6. è¯„è®ºæ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        plt.figure(figsize=(8, 6))
        plt.hist(self.user_features['è¯„è®ºæ•°é‡'], bins=30, alpha=0.7)
        plt.title('è¯„è®ºæ•°é‡åˆ†å¸ƒ')
        plt.xlabel('è¯„è®ºæ•°é‡')
        plt.ylabel('ç”¨æˆ·æ•°é‡')
        plt.show()

        # ç”Ÿæˆè¯äº‘å›¾
        self.generate_wordcloud()

    def visualize_results(self):
        """ç»“æœå¯è§†åŒ–"""
        print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. ç”¨æˆ·èšç±»åˆ†å¸ƒ
        cluster_counts = pd.Series(self.cluster_labels).value_counts().sort_index()
        axes[0, 0].pie(cluster_counts.values, labels=[f'ç°‡{i}' for i in cluster_counts.index],
                       autopct='%1.1f%%')
        axes[0, 0].set_title('ç”¨æˆ·èšç±»åˆ†å¸ƒ')

        # 2. æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ
        sentiment_counts = self.user_features['æƒ…æ„Ÿå€¾å‘'].value_counts()
        axes[0, 1].bar(sentiment_counts.index, sentiment_counts.values)
        axes[0, 1].set_title('æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('æƒ…æ„Ÿå€¾å‘')
        axes[0, 1].set_ylabel('ç”¨æˆ·æ•°é‡')

        # 3. æ´»è·ƒç­‰çº§åˆ†å¸ƒ
        activity_counts = self.user_features['æ´»è·ƒç­‰çº§'].value_counts()
        axes[0, 2].bar(activity_counts.index, activity_counts.values,
                       color=['red', 'orange', 'green'])
        axes[0, 2].set_title('æ´»è·ƒç­‰çº§åˆ†å¸ƒ')
        axes[0, 2].set_xlabel('æ´»è·ƒç­‰çº§')
        axes[0, 2].set_ylabel('ç”¨æˆ·æ•°é‡')

        # 4. èšç±»æ•£ç‚¹å›¾ (ä½¿ç”¨PCAé™ç»´)
        feature_cols = ['è¯„è®ºæ•°é‡', 'æ€»ç‚¹èµæ•°', 'æ€»å›å¤æ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'æ´»è·ƒåº¦å¾—åˆ†']
        X = self.user_features[feature_cols].fillna(0)

        if len(X) > 1:
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(StandardScaler().fit_transform(X))

            scatter = axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1],
                                         c=self.cluster_labels, cmap='viridis')
            axes[1, 0].set_title('ç”¨æˆ·èšç±»å¯è§†åŒ– (PCA)')
            axes[1, 0].set_xlabel('ä¸»æˆåˆ†1')
            axes[1, 0].set_ylabel('ä¸»æˆåˆ†2')
            plt.colorbar(scatter, ax=axes[1, 0])

        # 5. æ´»è·ƒåº¦vsæƒ…æ„Ÿå¾—åˆ†æ•£ç‚¹å›¾
        scatter2 = axes[1, 1].scatter(self.user_features['æ´»è·ƒåº¦å¾—åˆ†'],
                                      self.user_features['å¹³å‡æƒ…æ„Ÿå¾—åˆ†'],
                                      c=self.cluster_labels, cmap='viridis', alpha=0.6)
        axes[1, 1].set_title('æ´»è·ƒåº¦ vs æƒ…æ„Ÿå¾—åˆ†')
        axes[1, 1].set_xlabel('æ´»è·ƒåº¦å¾—åˆ†')
        axes[1, 1].set_ylabel('å¹³å‡æƒ…æ„Ÿå¾—åˆ†')

        # 6. è¯„è®ºæ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        axes[1, 2].hist(self.user_features['è¯„è®ºæ•°é‡'], bins=30, alpha=0.7)
        axes[1, 2].set_title('è¯„è®ºæ•°é‡åˆ†å¸ƒ')
        axes[1, 2].set_xlabel('è¯„è®ºæ•°é‡')
        axes[1, 2].set_ylabel('ç”¨æˆ·æ•°é‡')

        plt.tight_layout()
        plt.show()

        # ç”Ÿæˆè¯äº‘å›¾
        self.generate_wordcloud()

    def generate_wordcloud(self):
        """ç”Ÿæˆè¯äº‘å›¾"""
        print("ç”Ÿæˆè¯äº‘å›¾...")

        # åˆå¹¶æ‰€æœ‰è¯„è®ºæ–‡æœ¬
        all_text = ' '.join(self.user_features['æ‰€æœ‰è¯„è®º'].dropna())

        if all_text:
            # ç”Ÿæˆè¯äº‘
            wordcloud = WordCloud(
                font_path='simhei.ttf',  # éœ€è¦ä¸­æ–‡å­—ä½“æ–‡ä»¶
                width=800, height=400,
                background_color='white',
                max_words=100,
                collocations=False
            ).generate(all_text)

            plt.figure(figsize=(12, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('è¯„è®ºè¯äº‘å›¾', fontsize=16)
            plt.show()

    def classification_validation(self):
        """ä½¿ç”¨åˆ†ç±»ç®—æ³•éªŒè¯èšç±»æ•ˆæœ"""
        print("è¿›è¡Œåˆ†ç±»éªŒè¯...")

        # å‡†å¤‡åˆ†ç±»ç‰¹å¾
        feature_cols = ['è¯„è®ºæ•°é‡', 'æ€»ç‚¹èµæ•°', 'æ€»å›å¤æ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'æ´»è·ƒåº¦å¾—åˆ†']
        X = self.user_features[feature_cols].fillna(0)
        y = self.cluster_labels

        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # éšæœºæ£®æ—åˆ†ç±»
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_model.fit(X_train_scaled, y_train)
        rf_pred = rf_model.predict(X_test_scaled)

        print("\néšæœºæ£®æ—åˆ†ç±»ç»“æœ:")
        print(classification_report(y_test, rf_pred))

        # SVMåˆ†ç±»
        svm_model = SVC(kernel='rbf', random_state=42)
        svm_model.fit(X_train_scaled, y_train)
        svm_pred = svm_model.predict(X_test_scaled)

        print("\nSVMåˆ†ç±»ç»“æœ:")
        print(classification_report(y_test, svm_pred))

    def generate_user_tags(self):
        """ç”Ÿæˆç”¨æˆ·æ ‡ç­¾"""
        print("ç”Ÿæˆç”¨æˆ·æ ‡ç­¾...")

        # è®¡ç®—ç‚¹èµæ•°çš„åˆ†ä½æ•°é˜ˆå€¼
        likes_threshold = self.user_features['æ€»ç‚¹èµæ•°'].quantile(0.8)
        replies_threshold = self.user_features['æ€»å›å¤æ•°'].quantile(0.7)

        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆç»¼åˆæ ‡ç­¾
        def create_user_tag(row):
            tags = []

            # æ´»è·ƒåº¦æ ‡ç­¾
            tags.append(row['æ´»è·ƒç­‰çº§'])

            # æƒ…æ„Ÿæ ‡ç­¾
            tags.append(f"{row['æƒ…æ„Ÿå€¾å‘']}ç”¨æˆ·")

            # äº’åŠ¨æ ‡ç­¾
            if row['æ€»ç‚¹èµæ•°'] > likes_threshold:
                tags.append("é«˜äººæ°”")

            if row['æ€»å›å¤æ•°'] > replies_threshold:
                tags.append("äº’åŠ¨è¾¾äºº")

            # è¯„è®ºé¢‘ç‡æ ‡ç­¾
            if row['è¯„è®ºæ•°é‡'] >= 10:
                tags.append("æ´»è·ƒå‘è¨€")
            elif row['è¯„è®ºæ•°é‡'] >= 5:
                tags.append("é€‚åº¦å‘è¨€")
            else:
                tags.append("å¶å°”å‘è¨€")

            # èšç±»æ ‡ç­¾
            tags.append(f"ç±»å‹{row['èšç±»æ ‡ç­¾']}")

            return ', '.join(tags)

        self.user_features['ç”¨æˆ·æ ‡ç­¾'] = self.user_features.apply(create_user_tag, axis=1)

        # æ˜¾ç¤ºæ ‡ç­¾ç¤ºä¾‹
        print("\nç”¨æˆ·æ ‡ç­¾ç¤ºä¾‹:")
        sample_users = self.user_features[['ç”¨æˆ·å', 'ç”¨æˆ·æ ‡ç­¾']].head(10)
        for _, user in sample_users.iterrows():
            print(f"{user['ç”¨æˆ·å']}: {user['ç”¨æˆ·æ ‡ç­¾']}")

        # æ˜¾ç¤ºæ ‡ç­¾ç»Ÿè®¡
        print(f"\næ ‡ç­¾ç»Ÿè®¡:")
        print(f"- é«˜äººæ°”ç”¨æˆ·: {(self.user_features['æ€»ç‚¹èµæ•°'] > likes_threshold).sum()} äºº")
        print(f"- äº’åŠ¨è¾¾äºº: {(self.user_features['æ€»å›å¤æ•°'] > replies_threshold).sum()} äºº")
        print(f"- æ´»è·ƒå‘è¨€ç”¨æˆ·: {(self.user_features['è¯„è®ºæ•°é‡'] >= 10).sum()} äºº")

    def export_results(self, output_dir='./results'):
        """å¯¼å‡ºåˆ†æç»“æœåˆ°æŒ‡å®šç›®å½•"""
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # å¯¼å‡ºç”¨æˆ·ç”»åƒæ•°æ®
        user_profile_file = os.path.join(output_dir, 'user_profiling_results.csv')
        self.user_features.to_csv(user_profile_file, index=False, encoding='utf-8-sig')
        print(f"ç”¨æˆ·ç”»åƒç»“æœå·²ä¿å­˜åˆ°: {user_profile_file}")

        # å¯¼å‡ºèšç±»ç»Ÿè®¡æŠ¥å‘Š
        cluster_report_file = os.path.join(output_dir, 'cluster_analysis_report.txt')
        with open(cluster_report_file, 'w', encoding='utf-8') as f:
            f.write("Bç«™ç”¨æˆ·èšç±»åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            f.write(f"æ•°æ®æ¦‚å†µ:\n")
            f.write(f"- æ€»ç”¨æˆ·æ•°: {len(self.user_features)}\n")
            f.write(f"- èšç±»æ•°é‡: {len(set(self.cluster_labels))}\n\n")

            for cluster_id in range(max(self.cluster_labels) + 1):
                cluster_users = self.user_features[self.user_features['èšç±»æ ‡ç­¾'] == cluster_id]
                f.write(f"ç°‡ {cluster_id} åˆ†æ (ç”¨æˆ·æ•°: {len(cluster_users)}):\n")
                f.write(f"  - å¹³å‡è¯„è®ºæ•°: {cluster_users['è¯„è®ºæ•°é‡'].mean():.1f}\n")
                f.write(f"  - å¹³å‡ç‚¹èµæ•°: {cluster_users['æ€»ç‚¹èµæ•°'].mean():.1f}\n")
                f.write(f"  - å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {cluster_users['å¹³å‡æƒ…æ„Ÿå¾—åˆ†'].mean():.3f}\n")
                f.write(f"  - æ´»è·ƒç­‰çº§åˆ†å¸ƒ: {dict(cluster_users['æ´»è·ƒç­‰çº§'].value_counts())}\n")
                f.write(f"  - æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ: {dict(cluster_users['æƒ…æ„Ÿå€¾å‘'].value_counts())}\n\n")

        print(f"èšç±»åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {cluster_report_file}")

        # å¯¼å‡ºç”¨æˆ·æ ‡ç­¾ç»Ÿè®¡
        tag_stats_file = os.path.join(output_dir, 'user_tags_statistics.csv')
        if 'ç”¨æˆ·æ ‡ç­¾' in self.user_features.columns:
            tag_stats = self.user_features['ç”¨æˆ·æ ‡ç­¾'].value_counts().reset_index()
            tag_stats.columns = ['ç”¨æˆ·æ ‡ç­¾', 'ç”¨æˆ·æ•°é‡']
            tag_stats.to_csv(tag_stats_file, index=False, encoding='utf-8-sig')
            print(f"ç”¨æˆ·æ ‡ç­¾ç»Ÿè®¡å·²ä¿å­˜åˆ°: {tag_stats_file}")

        return output_dir


# ä½¿ç”¨ç¤ºä¾‹å’Œæ‰¹é‡å¤„ç†å‡½æ•°
def batch_process_pipeline(data_dir='./data', output_dir='./results', n_clusters=5):
    """å®Œæ•´çš„æ‰¹é‡å¤„ç†æµæ°´çº¿"""
    print("=" * 60)
    print("Bç«™è¯„è®ºç”¨æˆ·ç”»åƒåˆ†æç³»ç»Ÿ - æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("=" * 60)

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        profiler = BilibiliUserProfiler()

        # 1. æ‰¹é‡åŠ è½½æ•°æ®
        print("\næ­¥éª¤ 1: æ‰¹é‡åŠ è½½æ•°æ®")
        profiler.load_data(data_dir)

        # 2. æ•°æ®é¢„å¤„ç†
        print("\næ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†")
        profiler.data_preprocessing()

        # 3. ç‰¹å¾å·¥ç¨‹
        print("\næ­¥éª¤ 3: ç‰¹å¾å·¥ç¨‹")
        profiler.feature_engineering()

        # 4. ç”¨æˆ·èšç±»
        print("\næ­¥éª¤ 4: ç”¨æˆ·èšç±»")
        profiler.user_clustering(n_clusters=n_clusters)

        # 5. èšç±»ç»“æœåˆ†æ
        print("\næ­¥éª¤ 5: èšç±»ç»“æœåˆ†æ")
        profiler.analyze_clusters()

        # 6. ç”Ÿæˆç”¨æˆ·æ ‡ç­¾
        print("\næ­¥éª¤ 6: ç”Ÿæˆç”¨æˆ·æ ‡ç­¾")
        profiler.generate_user_tags()

        # 7. å¯è§†åŒ–åˆ†æ
        print("\næ­¥éª¤ 7: å¯è§†åŒ–åˆ†æ")
        profiler.visualize_results_v2()

        # 8. åˆ†ç±»éªŒè¯
        print("\næ­¥éª¤ 8: åˆ†ç±»ç®—æ³•éªŒè¯")
        profiler.classification_validation()

        # 9. å¯¼å‡ºç»“æœ
        print("\næ­¥éª¤ 9: å¯¼å‡ºåˆ†æç»“æœ")
        profiler.export_results(output_dir)

        print("\n" + "=" * 60)
        print("æ‰¹é‡å¤„ç†å®Œæˆ! ğŸ‰")
        print(f"ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
        print("=" * 60)

        return profiler

    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„æ˜¯å¦æ­£ç¡®")
        return None

def main():
    if os.path.exists('./data'):
        print(f"\næ£€æµ‹åˆ° ./data ç›®å½•å­˜åœ¨ï¼ŒåŒ…å«ä»¥ä¸‹æ–‡ä»¶:")
        csv_files = glob.glob('./data/*.csv')
        for file in csv_files:
            print(f"  - {os.path.basename(file)}")

        print(f"\nå¯ä»¥ç›´æ¥è¿è¡Œ: batch_process_pipeline() å¼€å§‹åˆ†æ")
        batch_process_pipeline()
    else:
        print(f"\nè¯·ç¡®ä¿å°†CSVæ–‡ä»¶æ”¾ç½®åœ¨ ./data ç›®å½•ä¸‹")


if __name__ == "__main__":
    main()